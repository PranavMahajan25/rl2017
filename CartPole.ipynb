{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Dense,Activation\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-09-24 22:56:47,826] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "gamma = 0.99\n",
    "\n",
    "def discount_rewards(r):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, r.size)):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating the agent\n",
    "\n",
    "class agent():\n",
    "    def __init__(self):\n",
    "        self.lr = 1e-2\n",
    "        self.s_size = 4\n",
    "        self.a_size = 2\n",
    "        self.h_size = 8\n",
    "        \n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(units = self.h_size, input_dim = self.s_size,kernel_initializer='random_normal',\n",
    "                bias_initializer='zeros'))\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(Dense(units=self.a_size,kernel_initializer='random_normal',\n",
    "                bias_initializer='zeros'))\n",
    "        self.model.add(Activation('softmax'))\n",
    "        self.model.compile(optimizer = 'adam', loss = 'mean_squared_error', metrics = ['accuracy'])\n",
    "        \n",
    "    def query_model(self,s):\n",
    "        #Keras expects each state observation to be a row (,4) Vector.\n",
    "        #Gym is spitting it out as a (4,) (or column) vector. \n",
    "        #So lets reshape this shit.\n",
    "        \n",
    "        s = np.reshape(s,[1,4])\n",
    "        \n",
    "        #This above line does the following\n",
    "        #new_s = np.zeros([1,4])\n",
    "        #new_s[0,0] = s[0]\n",
    "        #new_s[0,1] = s[1]\n",
    "        #new_s[0,2] = s[2]\n",
    "        #new_s[0,3] = s[3]\n",
    "        #s = new_s\n",
    "        \n",
    "        return self.model.predict(s)\n",
    "        \n",
    "    def optimize_model(self,state,action,reward):\n",
    "        \n",
    "        #write code to optimize model\n",
    "        return\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myAgent = agent()\n",
    "total_episodes = 5000\n",
    "max_ep = 99\n",
    "update_frequency = 5\n",
    "\n",
    "i = 0\n",
    "total_reward = []\n",
    "total_lenght = []\n",
    "\n",
    "while i <total_episodes:\n",
    "    s = env.reset()\n",
    "    running_reward = 0\n",
    "    ep_history = []\n",
    "    \n",
    "    for j in range(max_ep):\n",
    "        #Probabilistically pick an action given our network outputs.\n",
    "        a_dist = myAgent.query_model(s)\n",
    "        a = np.random.choice(a_dist[0],p = a_dist[0])\n",
    "        a = np.argmax(a_dist == a)\n",
    "        \n",
    "        s1,r,d,_ = env.step(a)\n",
    "        ep_history.append([s,a,r,s1])\n",
    "        \n",
    "        s = s1\n",
    "        running_reward = running_reward + r\n",
    "        \n",
    "        if d == True:\n",
    "            #update network\n",
    "            ep_history = np.array(ep_history)\n",
    "            ep_history[:,2] = discount_rewards(ep_history[:,2])\n",
    "            \n",
    "            myAgent.optimize_model(ep_history[:,0],ep_history[:,1],ep_history[:,2])\n",
    "            \n",
    "            total_reward.append(running_reward)\n",
    "            total_lenght.append(j)\n",
    "            break\n",
    "    \n",
    "    if i%100 == 0:\n",
    "        print(np.mean(total_reward[-100:]))\n",
    "    i = i + 1\n",
    "    \n",
    "            \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
